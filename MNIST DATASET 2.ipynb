{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0811d5b7-9a8c-47e3-8b2e-8ae9e4c2b785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81eb0b47-8e54-45b4-9430-4d9620b53e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "750e7e26-db09-4d1f-a221-5ee25722519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "#once we have loaded the dataset, we can easily extract the training and testing dataset with the built references\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "# by default, TF has training and testing datasets, but no validation sets\n",
    "# thus we must split it on our own\n",
    "# we start by defining the number of validation samples as a % of the train samples\n",
    "# this is also where we make use of mnist_info (we don't have to count the observations)\n",
    "\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "\n",
    "# let's cast this number to an integer, as a float may cause an error along the way\n",
    "# tf.cast(x,dtype)-casts (converts) a variable into a given data type\n",
    "\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "\n",
    "# once more, we'd prefer an integer (rather than the default float)\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "# normally, we would like to scale our data in some way to make the result more numerically stable\n",
    "# in this case we will simply prefer to have inputs between 0 and 1\n",
    "# let's define a function called: scale, that will take an MNIST image and its label\n",
    "\n",
    "def scale(image, label):\n",
    "# we make sure the value is a float\n",
    "    image = tf.cast(image, tf.float32)\n",
    "# since the possible values for the inputs are 0 to 255 (256 different shades of grey)\n",
    "# if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1\n",
    "    image /= 255. # the dot at the end means we want the result to be a float\n",
    "    return image, label\n",
    "\n",
    "# dataset.map(*function*) appplies a custom transformation to a given dataset. it takes as input a fn \n",
    "# which determines the transformation\n",
    "\n",
    "# we have already decided that we will get the validation data from mnist_train\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale) \n",
    "# this will scale the whole train dataset and store it in our new variable\n",
    "\n",
    "# finally, we scale and batch the test data\\n\",\n",
    "# we scale it so it has the same magnitude as the train and validation\n",
    "# there is no need to shuffle it, because we won't be training on the test data\n",
    "# there would be a single batch, equal to the size of the test data\n",
    "\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "# VID 379 preprocess the data-shuffle and Batch\n",
    "# SHUFFLING: Keeping the same infor but in a different order\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets\n",
    "# then we can't shuffle the whole dataset in one go because we can't fit it all in memory of the computer\n",
    "# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them\n",
    "# if BUFFER_SIZE=1 => no shuffling will actually happen\n",
    "# if BUFFER_SIZE >= num samples => shuffling will happen at once but uniformly\n",
    "# if 1< BUFFER_SIZE< num_samples, we will be optimizing the computational power of our computer \n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation\n",
    "# our validation data would be equal to 10% of the training set, which we've already calculated\n",
    "# we use the .take() method to take that many samples\n",
    "\n",
    "# finally, we create a batch with a batch size equal to the total number of validation samples\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset\n",
    "\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "# determine the batch size\n",
    "batch_size = 1000\n",
    "\n",
    "# we can also take advantage of the occasion to batch the train data\n",
    "# this would be very helpful when we train, as we would be able to iterate over the different batches\n",
    "\n",
    "# batch size = 1 = SGD\n",
    "# batch size = number of samples = (single batch) GD\n",
    "# dataset.batch(batch_size) a method that combines the consecutive elemrnts of a dataset into batches\n",
    "\n",
    "train_data = train_data.batch(batch_size) # this indicates to our model hw many samples it should take in each batch\n",
    "\n",
    "# Since we wont be back propagating in the validation data but only forward propagating we dont really need to batch\n",
    "# Recall that batching was useful in updating wt only once per batch which is like 100 samples rather than in every sample\n",
    "# hence reducing noise in the training update. so whenever we validate or test we simply forward propagate once\n",
    "# when batching we simply find the average loss and average accuracy. During validation and testing we want the exact values.\n",
    "# therefore we should take all the data at once. moreover when forward propagating we dont use that much computational power so\n",
    "# it is not expensive to calculate the exact values, however the model expects the validation in batch form too\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "# batch the test data\n",
    "test_data = test_data.batch(num_test_samples) # takes next batch (it is the only batch)\n",
    "\n",
    "# because as_supervized=True, we've got a 2-tuple structure(the mnist data is iterable and in 2-turple format)\n",
    "# so we must extract and convert the validation inputs and targets appropriately\n",
    "# our validation data must have the same shape and object properties as the train and test data \n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n",
    "\n",
    "# iter()-creates an object which can be iterated one element at a time (eg in a for or while loop). by default it will\n",
    "# make the dataset iterable but will not load any data\n",
    "# next()-loads the next (batch) elements of an iterable object. nd since there is only one batch it will load\n",
    "# the inputs and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2e74b49-4246-4ce4-b354-ddf8d99d8799",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 200\n",
    "\n",
    "model = tf.keras.Sequential([                          \n",
    "                            tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='tanh'), # 2nd hidden layer\n",
    "# the final layer is no different, we just make sure to activate it with softmax\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68a1d733-a408-4171-827d-53d4177194c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the optimizer we'd like to use\n",
    "# the loss function\n",
    "# and the metrics we are interested in obtaining at each iteration\n",
    "\n",
    "# model.compile(optimizer,loss)-configures the model for training\n",
    "# the string for optimizer are not case sensitive, so we can use small or capital letters\n",
    "\n",
    "# In tensorflow 2 there are 3 built-in variations of cross-entropy(CE) loss; \n",
    "# BINARY CE-used when there is binary encoding\n",
    "# CATEGORICAL CE-expects that you have one-hot encoded the targets\n",
    "# SPARSE CATEGORICAL CE- applies one-hot encoding\n",
    "\n",
    "   \n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c14df3bc-60cf-4ff9-8d63-3f789689fcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "54/54 - 30s - loss: 0.6084 - accuracy: 0.8365 - val_loss: 0.2332 - val_accuracy: 0.9325\n",
      "Epoch 2/5\n",
      "54/54 - 13s - loss: 0.1943 - accuracy: 0.9421 - val_loss: 0.1565 - val_accuracy: 0.9525\n",
      "Epoch 3/5\n",
      "54/54 - 12s - loss: 0.1356 - accuracy: 0.9591 - val_loss: 0.1232 - val_accuracy: 0.9643\n",
      "Epoch 4/5\n",
      "54/54 - 12s - loss: 0.1022 - accuracy: 0.9689 - val_loss: 0.0886 - val_accuracy: 0.9725\n",
      "Epoch 5/5\n",
      "54/54 - 11s - loss: 0.0797 - accuracy: 0.9759 - val_loss: 0.0790 - val_accuracy: 0.9757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29fefc683c8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the maximum number of epochs\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# we fit the model, specifying the\n",
    "# training data\n",
    "# the total number of epochs\n",
    "# and the validation data we just created ourselves in the format: (inputs,targets)\n",
    "\n",
    "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=validation_data, verbose =2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c79a546",
   "metadata": {},
   "source": [
    "## setting an early stopping mechanism with \"patience\" and repeating the TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "058a0a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "54/54 - 11s - loss: 5.3426e-04 - accuracy: 1.0000 - val_loss: 6.7930e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "54/54 - 12s - loss: 4.6758e-04 - accuracy: 1.0000 - val_loss: 7.1923e-04 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29fefa78828>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# That's where we train the model we have built\n",
    "# set the batch size\n",
    "batch_size = 1000\n",
    "\n",
    "# set a maximum number of training epochs\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# set an early stopping mechanism\n",
    "# let's set patience=2, to be a bit tolerant against random validation loss increases\n",
    "\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping()\n",
    "\n",
    "# we fit the model, specifying the\n",
    "# training data\n",
    "# the total number of epochs\n",
    "# and the validation data we just created ourselves in the format: (inputs,targets)\n",
    "\n",
    "model.fit(train_data, epochs=NUM_EPOCHS, callbacks=[early_stopping], validation_data = validation_data, verbose =2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be9eca8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 2s 2s/step - loss: 0.0796 - accuracy: 0.9809"
     ]
    }
   ],
   "source": [
    "# we can access the test accuracy using the method evaluate. \n",
    "# model.evaluate()- returns the loss value and metrics values fot the model in \"test mode\"\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "622d65b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.08. Test accuracy: 98.09%\n"
     ]
    }
   ],
   "source": [
    "# We can apply some nice formatting if we want to\n",
    "\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))\n",
    "\n",
    "# After we test the model conceptually we are not alloed to change it. if we start changing the model after this point, the test\n",
    "# data will no longer be a dataset the model has never seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eeb442",
   "metadata": {},
   "source": [
    "### FEEDLE WITH THE LEARNING RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "073c4b92-57e8-4c83-a579-b4abef47725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to define a custom optimizer (as we did in the TensorFlow intro)\n",
    "    \n",
    "# custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "#Then we change the respective argument in model.compile to reflect this\n",
    "# model.compile(optimizer=custom_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# While Adam adapts to the problem, if the orders of magnitude are too different, it may not have time to adjust accordingly. \n",
    "# We start overfitting before we can reach a neat solution.\n",
    "# Therefore, for this problem, even 0.02 is a **HIGH** starting learning rate. What if you try a learning rate of = 1?\n",
    "# It's a good practice to try 0.001, 0.0001, and 0.00001. If it makes no difference, pick whatever, \n",
    "# otherwise it makes sense to fiddle with the learning rate.\n",
    "\n",
    "# Adjust the learning rate. Try a value of 0.0001.\n",
    "# First, we have to define a custom optimizer (as we did in the TensorFlow intro)\n",
    "# custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "# Then we change the respective argument in model.compile to reflect this\n",
    "# model.compile(optimizer=custom_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Since the learning rate is lower than normal, we may need to adjust the max_epochs (to, say, 50).\n",
    "# The result is basically the same, but we reach it much slower.\n",
    "# While Adam adapts to the problem, if the orders of magnitude are too different, it may not have enough time to adjust accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617cdae5",
   "metadata": {},
   "source": [
    "### BATCH SIZE ADJUSTMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1526b41b-4f88-4e0e-8998-d30120658d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the batch size. Try a batch size of 10000. How does the required time change? What about the accuracy?\n",
    "\n",
    "# Change batch_size from 100 to 10000.\n",
    "\n",
    "# A bigger batch size results in slower training. That's what we expected from the theory. We are taking advantage of batching because of the amazing speed increase.\n",
    "\n",
    "# Notice that the validation accuracy starts from a low number and with 5 epochs actually **finishes** at a lower number. That's because there are **fewer** updates in a single epoch\n",
    "# Try a batch size of 30,000 or 50,000. That's very close to single batch GD for this problem. What do you think about the speed?You will need to change the max epochs to 100 (for instance), \n",
    "# as 5 epochs won't be enough to train the model. What do you think about the speed of optimization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56cc230",
   "metadata": {},
   "source": [
    "### ADJUSTING THE ACTIVATION FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39902641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiddle with the activation functions. Try applying a ReLu to the first hidden layer and tanh to the second one. The tanh activation is given by the string: 'tanh'\n",
    "# Analogically to the previous lecture, we can change the activation functions. This time though, we will use different activators for the different layers.\n",
    "# The result should not be significantly different. However, with different width and depth, that may change.\n",
    "# Additional exercise: Try to find a better combination of activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eeb8ba",
   "metadata": {},
   "source": [
    "### Feedle with the Activation fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a467646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiddle with the activation functions. Try applying sigmoid transformation to both layers. The sigmoid activation is given by the method: tf.nn.sigmoid()\n",
    "# Adjust the activations from 'relu' to 'sigmoid'\n",
    "# Generally, we should reach an inferior solution. That is because relu 'cleans' the noise in the data \n",
    "# (think about it - if a value is negative, relu filters it out, while if it is positive, it takes it into account). For the MNIST dataset, \n",
    "# we care only about the intensely black and white parts in the images of the digits, so such filtering proves beneficial.\\n\",\n",
    "# The sigmoid does not filter the signals as well as relu, but still reaches a respectable result (around 95%)\n",
    "# Try using softmax activations for all layers. How does the result change? Can you explain why that happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e50795",
   "metadata": {},
   "source": [
    "### ADJUSTING THE WIDTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2cb6e204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The *width* (the hidden layer size) of the algorithm. Try a hidden layer size of 200. How does the validation accuracy \n",
    "# of the model change? What about the time it took the algorithm to train?\n",
    "# The validation accuracy is significantly higher (as the algorithm with 50 hidden units was too simple of a model).\n",
    "#Naturally, it takes the algorithm much longer to train (unless early stopping is triggered too soon).\n",
    "# A hidden layer size of 500 (and not only) works even better.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b17ab82",
   "metadata": {},
   "source": [
    "### ADJUSTING THE DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25a5fe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The *depth* of the algorithm. Add another hidden layer to the algorithm. This is an extremely important exercise!\n",
    "# How does the validation accuracy change? What about the time it took the algorithm to train?\\n\",\n",
    "# Hint: Be careful with the shapes of the weights and the biases.\n",
    "#Adding another hidden layer to the algorithm is done in the same way as in the lecture.\n",
    "# tf.keras.layers.Dense(hidden_layer_size, activation='relu')\n",
    "# We can see that the accuracy of the model does not necessarily improve. This is an important lesson for us. Fiddling with a single hyperparameter may not be enough. \n",
    "# Sometimes, a deeper net needs to also be wider in order to have higher accuracy. Maybe you need more epochs?\n",
    "# ADDITIONAL TASK: Try this new model, but with a wider one (200-500 hidden units). Basically, combine this and the previous exercises**\\n\",\n",
    "# In any case, it takes longer for the algorithm to train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd1b936",
   "metadata": {},
   "source": [
    "### 98% model accuracy (ie using the test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15d77a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Achieving 98.5% accuracy with the methodology we've seen so far is extremely hard. A more realistic exercise would be to achieve 98%+ accuracy.\n",
    "# However, being pushed to the limit (trying to achieve 98.5%), you have probably learned a whole lot about the machine learning process.\n",
    "# Here is a link where you can check the results that some leading academics got on the MNIST (using different methodologies)\n",
    "# https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results\n",
    "# After some fine tuning, I decided to brute-force the algorithm and created 10 hidden layers with 5000 hidden units each.\n",
    "# hidden_layer_size = 5000\n",
    "# batch_size = 150\n",
    "# NUM_EPOCHS = 10\n",
    "# All activation functions are ReLu.\n",
    "# There are better solutions using this methodology, this one is just superior to the one in the lessons. \n",
    "# Due to the width and the depth of the algorithm, it took my computer 3 hours and 50 mins to train it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d79869",
   "metadata": {},
   "source": [
    "### ADJUSTING THE width and depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "713bff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The *width and depth* of the algorithm. Add as many additional layers as you need to reach 5 hidden layers. Moreover, adjust the width of the algorithm as you find suitable. \n",
    "# How does the validation accuracy change? What about the time it took the algorithm to train?\n",
    "\n",
    "#This exercise is pretty much the same as the previous one. However, it will get us to a much deeper net.\n",
    "# As we noted in the previous exercise, you a deeeper net may need to be wider to produce better results.\n",
    "# We tried with 1000 hidden units in each layer and 5 hidden layers.\n",
    "# The result (as you can see below) is that our model's training was going very well, until it overfit. It did so by quite a lot.\\n\",\n",
    "# It took my personal computer around 5-6 minutes to train the model.\n",
    "# What if you have more epochs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2547fbf",
   "metadata": {},
   "source": [
    "# BUSINESS CASE PREPROCESSING EXERCISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d7eda3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9400e+02 1.6200e+03 1.6200e+03 ... 5.0000e+00 9.2000e+01 0.0000e+00]\n",
      " [1.1430e+03 2.1600e+03 2.1600e+03 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [2.0590e+03 2.1600e+03 2.1600e+03 ... 0.0000e+00 3.8800e+02 0.0000e+00]\n",
      " ...\n",
      " [3.1134e+04 2.1600e+03 2.1600e+03 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [3.2832e+04 1.6200e+03 1.6200e+03 ... 0.0000e+00 9.0000e+01 0.0000e+00]\n",
      " [2.5100e+02 1.6740e+03 3.3480e+03 ... 0.0000e+00 0.0000e+00 1.0000e+00]]\n",
      "(14084, 12)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "raw_csv_data = np.loadtxt('Audiobooks_data.csv', delimiter=',')\n",
    "\n",
    "# Now you can work with the loaded data using NumPy's array operations\n",
    "print(raw_csv_data)  # Print the loaded data\n",
    "print(raw_csv_data.shape)  # Get the dimensions of the array\n",
    "\n",
    "# Breakdown:\n",
    "\n",
    "# np.loadtxt(): This function is specifically designed to load data from text files into a NumPy array.\n",
    "# 'Audiobooks_data.csv': This is the path to the CSV file you want to load. \n",
    "# Make sure the file is in the same directory as your Python script or provide the full path.\n",
    "# delimiter=',': This argument specifies that the values in the CSV file are separated by commas.\n",
    "\n",
    "# What this code does:\n",
    "\n",
    "# Reads the \"Audiobooks_data.csv\" file.\n",
    "# Interprets the data based on the comma delimiter.\n",
    "# Stores the loaded data into a NumPy array named raw_csv_data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f5805",
   "metadata": {},
   "source": [
    "## note that u can use this same code to preprocess any dataset that has two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d90e4343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.load:\n",
    "\n",
    "# Purpose: Primarily used to load NumPy arrays that have been saved in binary format (typically using np.save or np.savez).\n",
    "# Data Type: Specifically designed for loading NumPy arrays, which are efficient for numerical computations.   \n",
    "\n",
    "\n",
    "# Use np.load when:\n",
    "\n",
    "# You have data saved as NumPy arrays in binary format.\n",
    "# You primarily need to work with numerical data and require efficient computations.\n",
    "\n",
    "# Use pd.read_csv when:\n",
    "\n",
    "# You are working with CSV files.\n",
    "# You need to handle heterogeneous data types.\n",
    "# You require data cleaning, manipulation, and analysis capabilities beyond those offered by NumPy.\n",
    "\n",
    "# unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "\n",
    "# Explanation:\n",
    "\n",
    "# raw_csv_data: This is assumed to be a 2D NumPy array containing the data loaded from a CSV file.\n",
    "# [:,1:-1]: This is the core of the slicing operation:\n",
    "# :: This colon selects all rows of the raw_csv_data array.\n",
    "# 1: This indicates that the slicing starts from the second column (index 1, as Python uses zero-based indexing).\n",
    "# -1: This indicates the ending index for column selection. In Python, negative indices count from the end of the array or DataFrame. \n",
    "# -1 represents the last column. Therefore, 1:-1 selects all columns from the second column to the second-to-last column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b3475a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inputs are all columns in the csv, except for the first one [:,0] which is just the arbitrary customer IDs that bear no useful information\n",
    "# and the last one [:,-1] (which is our targets)\n",
    "\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "\n",
    "# The targets are in the last column. That's how datasets are conventionally organized.\n",
    "\n",
    "targets_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21ca440",
   "metadata": {},
   "source": [
    "## Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5416a876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many targets are 1 (meaning that the customer did convert)\n",
    "num_one_targets = int(np.sum(targets_all))\n",
    "\n",
    "# Set a counter for targets that are 0 (meaning that the customer did not convert)\n",
    "zero_targets_counter = 0\n",
    "# We want to create a balanced dataset, so we will have to remove some input/target pairs.\n",
    "# Declare a variable that will do that:\n",
    "\n",
    "indices_to_remove = []\n",
    "# Count the number of targets that are 0.\n",
    "# Once there are as many 0s as 1s, mark entries where the target is 0.\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "            \n",
    "# Create two new variables, one that will contain the inputs, and one that will contain the targets.\n",
    "# We delete all indices that we marked \\\"to remove\\\" in the loop above.\n",
    "\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
    "\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe3c5b7",
   "metadata": {},
   "source": [
    "## Standardize the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45110ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result will be interesting as stdizing them will grtly improve the algo\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb35d472",
   "metadata": {},
   "source": [
    "## Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d53d3522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the data was collected it was actually arranged by date\n",
    "# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.\n",
    "# Since we will be batching, we want the data to be as randomly spread out as possible\n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "# Use the shuffled indices to shuffle the inputs and targets.\n",
    "\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c5cf5a",
   "metadata": {},
   "source": [
    "## Split the dataset into train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4f846e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1796.0 3579 0.5018161497625034\n",
      "221.0 447 0.49440715883668906\n",
      "220.0 448 0.49107142857142855\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of samples\n",
    "\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "# Count the samples in each subset, assuming we want 80-10-10 distribution of training, validation, and test.\n",
    "# Naturally, the numbers are integers.\n",
    "\n",
    "train_samples_count = int(0.8 * samples_count)\n",
    "validation_samples_count = int(0.1 * samples_count)\n",
    "\n",
    "# The 'test' dataset contains all remaining data.\\n\",\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "# Create variables that record the inputs and targets for training\n",
    "# In our shuffled dataset, they are the first train_samples_count observations\n",
    "\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for validation.\n",
    "# They are the next validation_samples_count observations, folllowing the train_samples_count we already assigned\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for test.\n",
    "# They are everything that is remaining.\n",
    "\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
    "    \n",
    "# We balanced our dataset to be 50-50 (for targets 0 and 1), but the training, validation, and test were\n",
    "# taken from a shuffled dataset. Check if they are balanced, too. Note that each time you rerun this code\n",
    "# you will get different values, as each time they are shuffled randomly\n",
    "# Normally you preprocess ONCE, so you need not rerun this code once it is done\n",
    "# If you rerun this whole sheet, the npzs will be overwritten with your newly preprocessed data.\n",
    "# Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test\n",
    "\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)\n",
    "\n",
    "# THE PRIORS LOOK OK AS THEY ARE almost ALL EQUAL. NOTE THAT 52% OR 55% FOR TWO CLASSES ARE ALSO FINE HWWEVER WE WANT TO BE\n",
    "# CLOSE TO 50% AS POSSIBLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3324533e",
   "metadata": {},
   "source": [
    "## Save the three datasets in .npz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e85167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the three datasets in .npz.\n",
    "    \n",
    "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa873104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc3e25e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
