{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "807d6d9e",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification\n",
    "\n",
    "We'll apply all the knowledge from the lectures in this section to write a deep neural network. The problem we've chosen is referred to as the \\\"Hello World\\\" of deep learning because for most students it is the first deep learning algorithm they see.\n",
    "   \n",
    "The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as covolutional neural networks (CNNs).\n",
    "\n",
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image)\n",
    "    \"The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes.\n",
    "   \n",
    "Our goal would be to build a neural network with 2 hidden layers.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a6fc1f",
   "metadata": {},
   "source": [
    "### VID 376 Importing relevant packages and loading the data\n",
    "### Import relevant library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e459caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "358d23c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFLow includes a data provider for MNIST that we'll use.\\n\",\n",
    "# It comes with the tensorflow-datasets module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb2a180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ce4ae3",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2487afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it)\n",
    "# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument\n",
    "# there are other arguments we can specify, which we can find useful\n",
    "# mnist_dataset = tfds.load(name='mnist', as_supervised=True)\n",
    "# mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "# with_info=True will also provide us with a tuple containing information about the version, features, number of samples\n",
    "# we will use this information a bit below and we will store it in mnist_info\n",
    "# as_supervised=True will load the dataset in a 2-tuple structure (input, target)\n",
    "# alternatively, as_supervised=False, would return a dictionary\n",
    "# obviously we prefer to have our inputs and targets separated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f571a",
   "metadata": {},
   "source": [
    "## VID 377 preprocessing the data- Create a validation set and scale it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75f6d94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "#once we have loaded the dataset, we can easily extract the training and testing dataset with the built references\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "# by default, TF has training and testing datasets, but no validation sets\n",
    "# thus we must split it on our own\n",
    "# we start by defining the number of validation samples as a % of the train samples\n",
    "# this is also where we make use of mnist_info (we don't have to count the observations)\n",
    "\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "\n",
    "# let's cast this number to an integer, as a float may cause an error along the way\n",
    "# tf.cast(x,dtype)-casts (converts) a variable into a given data type\n",
    "\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "\n",
    "# once more, we'd prefer an integer (rather than the default float)\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "# normally, we would like to scale our data in some way to make the result more numerically stable\n",
    "# in this case we will simply prefer to have inputs between 0 and 1\n",
    "# let's define a function called: scale, that will take an MNIST image and its label\n",
    "\n",
    "def scale(image, label):\n",
    "# we make sure the value is a float\n",
    "    image = tf.cast(image, tf.float32)\n",
    "# since the possible values for the inputs are 0 to 255 (256 different shades of grey)\n",
    "# if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1\n",
    "    image /= 255. # the dot at the end means we want the result to be a float\n",
    "    return image, label\n",
    "\n",
    "# dataset.map(*function*) appplies a custom transformation to a given dataset. it takes as input a fn \n",
    "# which determines the transformation\n",
    "\n",
    "# we have already decided that we will get the validation data from mnist_train\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale) \n",
    "# this will scale the whole train dataset and store it in our new variable\n",
    "\n",
    "# finally, we scale and batch the test data\\n\",\n",
    "# we scale it so it has the same magnitude as the train and validation\n",
    "# there is no need to shuffle it, because we won't be training on the test data\n",
    "# there would be a single batch, equal to the size of the test data\n",
    "\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "# VID 379 preprocess the data-shuffle and Batch\n",
    "# SHUFFLING: Keeping the same infor but in a different order\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets\n",
    "# then we can't shuffle the whole dataset in one go because we can't fit it all in memory of the computer\n",
    "# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them\n",
    "# if BUFFER_SIZE=1 => no shuffling will actually happen\n",
    "# if BUFFER_SIZE >= num samples => shuffling will happen at once but uniformly\n",
    "# if 1< BUFFER_SIZE< num_samples, we will be optimizing the computational power of our computer \n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation\n",
    "# our validation data would be equal to 10% of the training set, which we've already calculated\n",
    "# we use the .take() method to take that many samples\n",
    "\n",
    "# finally, we create a batch with a batch size equal to the total number of validation samples\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset\n",
    "\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "# determine the batch size\n",
    "batch_size = 100\n",
    "\n",
    "# we can also take advantage of the occasion to batch the train data\n",
    "# this would be very helpful when we train, as we would be able to iterate over the different batches\n",
    "\n",
    "# batch size = 1 = SGD\n",
    "# batch size = number of samples = (single batch) GD\n",
    "# dataset.batch(batch_size) a method that combines the consecutive elemrnts of a dataset into batches\n",
    "\n",
    "train_data = train_data.batch(batch_size) # this indicates to our model hw many samples it should take in each batch\n",
    "\n",
    "# Since we wont be back propagating in the validation data but only forward propagating we dont really need to batch\n",
    "# Recall that batching was useful in updating wt only once per batch which is like 100 samples rather than in every sample\n",
    "# hence reducing noise in the training update. so whenever we validate or test we simply forward propagate once\n",
    "# when batching we simply find the average loss and average accuracy. During validation and testing we want the exact values.\n",
    "# therefore we should take all the data at once. moreover when forward propagating we dont use that much computational power so\n",
    "# it is not expensive to calculate the exact values, however the model expects the validation in batch form too\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "# batch the test data\n",
    "test_data = test_data.batch(num_test_samples) # takes next batch (it is the only batch)\n",
    "\n",
    "# because as_supervized=True, we've got a 2-tuple structure(the mnist data is iterable and in 2-turple format)\n",
    "# so we must extract and convert the validation inputs and targets appropriately\n",
    "# our validation data must have the same shape and object properties as the train and test data \n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n",
    "\n",
    "# iter()-creates an object which can be iterated one element at a time (eg in a for or while loop). by default it will\n",
    "# make the dataset iterable but will not load any data\n",
    "# next()-loads the next (batch) elements of an iterable object. nd since there is only one batch it will load\n",
    "# the inputs and targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c7b3aa",
   "metadata": {},
   "source": [
    "### VID 381 MNIST; outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00cf4937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When thinking about a deep learning algorithm, we mostly imagine building the model.\n",
    "\n",
    "# input_size = 784\n",
    "# output_size = 10\n",
    "# Use same hidden layer size for both hidden layers (Not a necessity though)\n",
    "# underlying assumption is that all hidden layers are of same size\n",
    "# Recall that width and depth are hyper parameters\n",
    "\n",
    "# hidden_layer_size = 50\n",
    "\n",
    "# the underlying assumption is that all hidden layers are of the same size alternatively i can create hidden layers with\n",
    "# different width and see if they work better for ur particular problem\n",
    "\n",
    "# define how the model will look like\n",
    "# tf.keras.Sequential()-fn that lays down the model(used to stack layers)\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "# the first layer (the input layer)\n",
    "# each observation is 28x28x1 pixels, therefore it is a tensor of rank 3\n",
    "# so we must flatten the images\n",
    "# there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (none)\n",
    "# or (28x28x1,) = (784,) vector\n",
    "# tf.keras.layers.Flatten(original shape)-transforms (flattens) a tensor into a vector\n",
    "# this allows us to actually create a feed forward neural network\n",
    "                          \n",
    "                            # tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "    \n",
    "# tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "# it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "    \n",
    "# tf.keras.layers.Dense(output size)-takes the inputs, provided to the model and calculates the dot product of the inputs\n",
    "# and wts and adds the bias. this is also where we can apply the activation fn\n",
    "                            \n",
    "                            # tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "                            # tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "# the final layer is no different, we just make sure to activate it with softmax\n",
    "                            # tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "                            # ])\n",
    "        \n",
    "# YOU CAN STACK MANY LAYERS AS YOU WANT USING THIS STRUCTURE: 1 WIDTH 2 DEPTH 3 ACTIVATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbf1750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVING ALL COMMENTS FROM ABOVE\n",
    "\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 200\n",
    "\n",
    "model = tf.keras.Sequential([                          \n",
    "                            tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "# the final layer is no different, we just make sure to activate it with softmax\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e16d6",
   "metadata": {},
   "source": [
    "### VID 382 select the loss fn and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "209caf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the optimizer we'd like to use\n",
    "# the loss function\n",
    "# and the metrics we are interested in obtaining at each iteration\n",
    "\n",
    "# model.compile(optimizer,loss)-configures the model for training\n",
    "# the string for optimizer are not case sensitive, so we can use small or capital letters\n",
    "\n",
    "# In tensorflow 2 there are 3 built-in variations of cross-entropy(CE) loss; \n",
    "# BINARY CE-used when there is binary encoding\n",
    "# CATEGORICAL CE-expects that you have one-hot encoded the targets\n",
    "# SPARSE CATEGORICAL CE- applies one-hot encoding\n",
    "\n",
    "   \n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc77e32",
   "metadata": {},
   "source": [
    "### VID 383 MNIST: LEARNING\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ce75f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 103s - loss: 0.2725 - accuracy: 0.9194 - val_loss: 0.1300 - val_accuracy: 0.9623\n",
      "Epoch 2/5\n",
      "540/540 - 18s - loss: 0.1030 - accuracy: 0.9691 - val_loss: 0.0884 - val_accuracy: 0.9745\n",
      "Epoch 3/5\n",
      "540/540 - 14s - loss: 0.0691 - accuracy: 0.9781 - val_loss: 0.0725 - val_accuracy: 0.9782\n",
      "Epoch 4/5\n",
      "540/540 - 17s - loss: 0.0531 - accuracy: 0.9829 - val_loss: 0.0481 - val_accuracy: 0.9873\n",
      "Epoch 5/5\n",
      "540/540 - 13s - loss: 0.0381 - accuracy: 0.9879 - val_loss: 0.0477 - val_accuracy: 0.9867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d5879c3390>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we train the model we have built\n",
    "\n",
    "# determine the maximum number of epochs\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# we fit the model, specifying the\n",
    "# training data\n",
    "# the total number of epochs\n",
    "# and the validation data we just created ourselves in the format: (inputs,targets)\n",
    "\n",
    "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=validation_data, verbose =2)\n",
    "\n",
    "# WHAT HAPPENS INSIDE AN EPOCH\n",
    "#1 At the beginning of each epoch, the training loss will be set to 0\n",
    "#2 The algorithm will iterate over a preset no of batches, all from train_data\n",
    "#3 The wts and biases will be updated as many times as there are batches\n",
    "#4 We will get value for the loss fn, indicating hw the training is going\n",
    "#5 We willalso see a training accuracy\n",
    "#6 At the end of the epoch, the algorithm will forward propagate the whole validation set\n",
    "\n",
    "# When we reach the maximum number of epochs the training will be over\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b65a42d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we have number of epoch(epoch 1/5)\n",
    "# 2nd is the number of batches (540/540)\n",
    "# 3rd is time it took the epoch to conclude\n",
    "# 4th is the training loss. it should be compared to other training loss across epochs,in our case it is mostly decreasing.\n",
    "# notice that it didnt change too much bc even after the first epoch we have already had 540 diff wts and bias updates\n",
    "# one for each batch\n",
    "# the next is ACURRACY. it shows in what percent of the cases our output were equal to the target. logically it follows the trend\n",
    "# of the loss as they both represent how well the outputs match the targets\n",
    "# lastly is the LOSS nd ACCURACY for the VALIDATION dataset. usually we keep an eye on the validation loss(or set early stopping mechnism)\n",
    "# to determine wether our model is overfitting\n",
    "# the validation accuracy = TRUE ACCURACY OF THE MODEL for the epoch. this is bc the training acuracy is THE AVRGE ACURACY \n",
    "# ACROSS BATCHES WHILE THE VALIDATION ACURACY IS THAT OF THE WHOLE VALIDATION SET\n",
    "# TO acess the overall acuracy of our model we look at the validation acuracy of the last epoch(97%).\n",
    "# this is a good result already\n",
    "\n",
    "# lets feedle with some hyperparameters like the hiden_layer_size. increasing the hidden_layer_size causes the accuracy\n",
    "# of validation set to increase and the training time increased too\n",
    "\n",
    "# increasing the hidden_layer_size to 200 causes the accuracy to increase nd the training time to reduce a bit from 31 to 14sec\n",
    "# WIDTH IS THE HIDDEN LAYER size\n",
    "# DEPTH IS THE NUMBER OF HIDDEN LAYER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e203f87",
   "metadata": {},
   "source": [
    "### VID 385 TESTING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f788b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we got 98% for the validation accuracy. so we have to test the model on the test dataset bc the final accuracy of the model\n",
    "# comes from forward propagating the test dataset. the reason is we may have overfit by trying differnt combinations of hyper-\n",
    "# parameters. so therefor to check the accuracy of the model we use the dateset the model has not seen before (test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1537602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 3s 3s/step - loss: 0.0726 - accuracy: 0.9776"
     ]
    }
   ],
   "source": [
    "# we can access the test accuracy using the method evaluate. \n",
    "# model.evaluate()- returns the loss value and metrics values fot the model in \"test mode\"\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e006ef86",
   "metadata": {},
   "source": [
    "### NOTE: the main aim of the test dataset is to simulate model deployment. if we get 50 to 60% accuracy we will \n",
    "### know for sure that our model has overfit and it will fail miserably in real life. Getting a value very close to the validation\n",
    "### accuracy shows we have not overfit. the test accuracy is the accuracy we expect the model to produce \n",
    "### when deployed in the real world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f195570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.32. Test accuracy: 81.25%\n"
     ]
    }
   ],
   "source": [
    "# We can apply some nice formatting if we want to\n",
    "\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))\n",
    "\n",
    "# After we test the model conceptually we are not alloed to change it. if we start changing the model after this point, the test\n",
    "# data will no longer be a dataset the model has never seen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd18011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8d4e6ad",
   "metadata": {},
   "source": [
    "## SECTION 53 DEEP LEARNING BUSINESS CASE EXAMPLE\n",
    "## VID 389"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b9f36c",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "#### You are given data from an Audiobook App. Logically, it relates to the audio versions of books ONLY. Each customer in the database has made a purchase at least once, that's why he/she is in the database. We want to create a machine learning algorithm based on our available data that can predict if a customer will buy again from the Audiobook company\n",
    "    \n",
    "#### The main idea is that if a customer has a low probability of coming back, there is no reason to spend any money on advertising to him/her. If we can focus our efforts SOLELY on customers that are likely to convert again, we can make great savings. Moreover, this model can identify the most important metrics for a customer to come back again. Identifying new customers creates value and growth opportunities.\n",
    "    \n",
    "#### You have a .csv summarizing the data. There are several variables: Customer ID, ), Book length overall (sum of the minute length of all purchases), Book length avg (average length in minutes of all purchases), Price paid_overall (sum of all purchases) ,Price Paid avg (average of all purchases), Review (a Boolean variable whether the customer left a review), Review out of 10 (if the customer left a review, his/her review out of 10, Total minutes listened, Completion (from 0 to 1), Support requests (number of support requests; everything from forgotten password to assistance for using the App), and Last visited minus purchase date (in days).\n",
    "\n",
    "#### These are the inputs (excluding customer ID, as it is completely arbitrary. It's more like a name, than a number).\n",
    "\n",
    "#### The targets are a Boolean variable (0 or 1). We are taking a period of 2 years in our inputs, and the next 6 months as targets. So, in fact, we are predicting if: based on the last 2 years of activity and engagement, a customer will convert in the next 6 months. 6 months sounds like a reasonable time. If they don't convert after 6 months, chances are they've gone to a competitor or didn't like the Audiobook way of digesting information.\n",
    "\n",
    "#### The task is simple: create a machine learning algorithm, which is able to predict if a customer will buy again.\n",
    "#### This is a classification problem with two classes: won't buy and will buy, represented by 0s and 1s. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c8df7a",
   "metadata": {},
   "source": [
    "### Preprocess the data. Balance the dataset. Create 3 datasets: training, validation, and test. \n",
    "### Save the newly created sets in a tensor friendly format (e.g. *.npz). Since we are dealing with real life data, we will need to preprocess it a bit. This is the relevant code, which is not that hard, but is crucial to creating a good model.\n",
    "    \n",
    "###   If you want to know how to do that, go through the code with comments. In any case, this should do the trick for most datasets organized in the way: many inputs, and then 1 cell containing the targets (supervized learning datasets). Keep in mind that a specific problem may require additional preprocessing.\n",
    "    \n",
    "### Note that we have removed the header row, which contains the names of the categories. We simply want the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b089ad",
   "metadata": {},
   "source": [
    "## Extract the data from the csv\n",
    "\n",
    "## note that u can use this same code to preprocess any dataset that has two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2777901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "# We will use the sklearn preprocessing library, as it will be easier to standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73f65008",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1d6548c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9400e+02, 1.6200e+03, 1.6200e+03, ..., 5.0000e+00, 9.2000e+01,\n",
       "        0.0000e+00],\n",
       "       [1.1430e+03, 2.1600e+03, 2.1600e+03, ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00],\n",
       "       [2.0590e+03, 2.1600e+03, 2.1600e+03, ..., 0.0000e+00, 3.8800e+02,\n",
       "        0.0000e+00],\n",
       "       ...,\n",
       "       [3.1134e+04, 2.1600e+03, 2.1600e+03, ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00],\n",
       "       [3.2832e+04, 1.6200e+03, 1.6200e+03, ..., 0.0000e+00, 9.0000e+01,\n",
       "        0.0000e+00],\n",
       "       [2.5100e+02, 1.6740e+03, 3.3480e+03, ..., 0.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_csv_data\n",
    "\n",
    "# The inputs are all columns in the csv, except for the first one [:,0]\n",
    "# (which is just the arbitrary customer IDs that bear no useful information) and the last one [:,-1] (which is our targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0acc1ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_inputs_all = raw_csv_data[:,1:-1] # this excludes customer ID column and target column\n",
    "\n",
    "# The targets are in the last column. That's how datasets are conventionally organized.\n",
    "targets_all = raw_csv_data[:,-1] # this outputs only the target columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a336a1",
   "metadata": {},
   "source": [
    "## Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8155b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many targets are 1 (meaning that the customer did convert)\n",
    "num_one_targets = int(np.sum(targets_all))\n",
    "\n",
    "# Set a counter for targets that are 0 (meaning that the customer did not convert)\n",
    "zero_targets_counter = 0\n",
    "\n",
    "# We want to create a balanced dataset, so we will have to remove some input/target pairs.\n",
    "# Declare a variable that will do that:\n",
    "indices_to_remove = []\n",
    "\n",
    "# Count the number of targets that are 0.\n",
    "# Once there are as many 0s as 1s,(i will know the indices of all data point to be removed) mark entries where the target is 0.\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "\n",
    "# append() is a method that adds (appends) an object to a list\n",
    "\n",
    "# THE VAR INDICES_TO_REMOVE WILL CONTAIN THE INDICES OF ALL TARGTES WE WONT NEED ND DELETING THEM WILL BALANCE THE DATASET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0be0d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two new variables, one that will contain the inputs, and one that will contain the targets.\n",
    "# We delete all indices that we marked \\\"to remove\\\" in the loop above.\n",
    "\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
    "   \n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe49b4cf",
   "metadata": {},
   "source": [
    "## Standardize (scale) the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92112473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the only place we use sklearn functionality. We will take advantage of its preprocessing capabilities\n",
    "# stdizing the inputs will grtly improve the algo\n",
    "# At the end of the business case, you can try to run the algorithm WITHOUT this line of code.\n",
    "# The result will be interesting.\n",
    "\n",
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)\n",
    "\n",
    "# preprocessing.scale (x) is a method that stdizes the dataset along each variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f4ada4",
   "metadata": {},
   "source": [
    "## Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d69fb853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A little trick is to shuffle the inputs and the targets(since we will be batching). we keep the same information but in a radom order\n",
    "\n",
    "# When the data was collected it was actually arranged by date\n",
    "# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.\n",
    "\n",
    "# Since we will be batching, we want the data to be as randomly spread out as possible\n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "# np.random.shuffle(x) is a method that shuffles the numbers in a given sequence\n",
    "\n",
    "# Use the shuffled indices to shuffle the inputs and targets.\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b284f52",
   "metadata": {},
   "source": [
    "## Split the dataset into train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa62268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of samples\n",
    "\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "# Count the samples in each subset, assuming we want 80-10-10 distribution of training, validation, and test.\\n\",\n",
    "# Naturally, we want to make sure the numbers are integers\n",
    "\n",
    "train_samples_count = int(0.8 * samples_count)\n",
    "validation_samples_count = int(0.1 * samples_count)\n",
    "\n",
    "# The 'test' dataset contains all remaining data\n",
    "\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "\n",
    "# Create variables that record the inputs and targets for training\\n\",\n",
    "\n",
    "# In our shuffled dataset, they are the first \\\"train_samples_count\\\" observations\\n\",\n",
    "\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for validation.\n",
    "# They are the next \\\"validation_samples_count\\\" observations, folllowing the \\\"train_samples_count\\\" we already assigned\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for test\n",
    "# They are everything that is remaining\n",
    "\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cccecfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1767.0 3579 0.49371332774518023\n",
      "231.0 447 0.5167785234899329\n",
      "239.0 448 0.5334821428571429\n"
     ]
    }
   ],
   "source": [
    "# We balanced our dataset to be 50-50 (for targets 0 and 1), but the training, validation, and test were\n",
    "# taken from a shuffled dataset. Check if they are balanced, too. Note that each time you rerun this code, \n",
    "# you will get different values, as each time they are shuffled randomly.\n",
    "# Normally you preprocess ONCE, so you need not rerun this code once it is done.\n",
    "# If you rerun this whole sheet, the npzs will be overwritten with your newly preprocessed data.\n",
    "    \n",
    "# Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test.\n",
    "\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a71f53",
   "metadata": {},
   "source": [
    "## Save the three datasets in .npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "277f4c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the three datasets in .npz.\n",
    "    \n",
    "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c68020",
   "metadata": {},
   "source": [
    "## VID 391 BUSINESS CASE: LOAD THE PREPROCESSED DATA\n",
    "## The task is simple: create a machine learning algorithm, which is able to predict if a customer will buy again.\n",
    "## This is a classification problem with two classes: won't buy and will buy, represented by 0s and 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "35fed019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our deep learning net has 10 units(ie input from our csv) and 2 output nodes as there are only 2 possibilities 0s nd 1s\n",
    "# we will build a net with 2 hidden layers, the no of units in each layer will be 50 (but we can change it) as 50 provide\n",
    "# enough complexity so we expect the algo to be much more sorphisticated than a linear or logistic regression and we dont want\n",
    "# to put too many units initially so we can complete learning as fast as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3feda5a",
   "metadata": {},
   "source": [
    "## Create the machine learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a95b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary library\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f06dcec",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "042a8247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a temporary variable npz, where we will store each of the three Audiobooks datasets\n",
    "\n",
    "npz = np.load('Audiobooks_data_train.npz')\n",
    "\n",
    "# we extract the inputs using the keyword under which we saved them\n",
    "# to ensure that they are all floats, let's also take care of that\n",
    "train_inputs = npz['inputs'].astype(np.float)\n",
    "\n",
    "# np.ndarray.astype()- creates a copy of the array, cast to a specific type\n",
    "\n",
    "# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them)\n",
    "train_targets = npz['targets'].astype(np.int)\n",
    "\n",
    "# we load the validation data in the temporary variable\n",
    "npz = np.load('Audiobooks_data_validation.npz')\n",
    "\n",
    "# we can load the inputs and the targets in the same line\n",
    "validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "\n",
    "# we load the test data in the temporary variable\n",
    "npz = np.load('Audiobooks_data_test.npz')\n",
    "\n",
    "# we create 2 variables that will contain the test inputs and the test targets\n",
    "test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5048c415",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3a32bc8",
   "metadata": {},
   "source": [
    "# Model\n",
    "### Outline, optimizers, loss, early stopping and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1eef9917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input and output sizes\n",
    "input_size = 10\n",
    "output_size = 2\n",
    "\n",
    "# Use same hidden layer size for both hidden layers. Not a necessity.\n",
    "hidden_layer_size = 50\n",
    "\n",
    "# tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "# it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "# the final layer is no different, we just make sure to activate it with softmax\n",
    "\n",
    "# define how the model will look like\n",
    "\n",
    "model = tf.keras.Sequential([ \n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax') # output laye\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195b5c7f",
   "metadata": {},
   "source": [
    "### The Flatten layer in machine learning, particularly in deep learning, is primarily used to convert multi-dimensional data into a one-dimensional array. This transformation is crucial when transitioning from convolutional or recurrent layers to fully connected (dense) layers in neural networks.   \n",
    "\n",
    "Transitioning from Convolutional Layers to Dense Layers:\n",
    "\n",
    "Convolutional layers, commonly used in image processing and computer vision, extract features from images in the form of multi-dimensional feature maps.   \n",
    "Dense layers, however, require one-dimensional input vectors.\n",
    "The Flatten layer acts as a bridge between these two types of layers by reshaping the multi-dimensional feature maps into a single, long vector.   \n",
    "Simplifying Data for Dense Layers:\n",
    "\n",
    "Dense layers operate on vectors of features. By flattening the input, we remove any spatial or temporal structure present in the original data and convert it into a format suitable for processing by densely connected neurons.   \n",
    "Reducing Model Complexity:\n",
    "\n",
    "Flattening can reduce the dimensionality of the data, which can help reduce the number of parameters in subsequent dense layers. This can be beneficial in preventing overfitting, especially in deep networks with many parameters.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0b4ae669",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Choose the optimizer and the loss function\n",
    "# we define the optimizer we'd like to use\n",
    "# the loss function and the metrics we are interested in obtaining at each iteration\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# sparse_categorical_crossentropy is used to ensure that our integer targets are one-hot encoded appropriately when calculating\n",
    "# the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7648c99",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "253678eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3579 samples, validate on 447 samples\n",
      "Epoch 1/100\n",
      "3579/3579 - 2s - loss: 0.5854 - accuracy: 0.6843 - val_loss: 0.5219 - val_accuracy: 0.7383\n",
      "Epoch 2/100\n",
      "3579/3579 - 0s - loss: 0.4702 - accuracy: 0.7550 - val_loss: 0.4542 - val_accuracy: 0.7718\n",
      "Epoch 3/100\n",
      "3579/3579 - 0s - loss: 0.4225 - accuracy: 0.7784 - val_loss: 0.4232 - val_accuracy: 0.7651\n",
      "Epoch 4/100\n",
      "3579/3579 - 0s - loss: 0.3986 - accuracy: 0.7832 - val_loss: 0.3919 - val_accuracy: 0.8031\n",
      "Epoch 5/100\n",
      "3579/3579 - 0s - loss: 0.3812 - accuracy: 0.7907 - val_loss: 0.3832 - val_accuracy: 0.7696\n",
      "Epoch 6/100\n",
      "3579/3579 - 0s - loss: 0.3720 - accuracy: 0.7955 - val_loss: 0.3634 - val_accuracy: 0.8277\n",
      "Epoch 7/100\n",
      "3579/3579 - 0s - loss: 0.3648 - accuracy: 0.8011 - val_loss: 0.3553 - val_accuracy: 0.8031\n",
      "Epoch 8/100\n",
      "3579/3579 - 0s - loss: 0.3546 - accuracy: 0.8080 - val_loss: 0.3483 - val_accuracy: 0.8188\n",
      "Epoch 9/100\n",
      "3579/3579 - 0s - loss: 0.3541 - accuracy: 0.8044 - val_loss: 0.3566 - val_accuracy: 0.7919\n",
      "Epoch 10/100\n",
      "3579/3579 - 0s - loss: 0.3498 - accuracy: 0.8108 - val_loss: 0.3480 - val_accuracy: 0.8233\n",
      "Epoch 11/100\n",
      "3579/3579 - 0s - loss: 0.3446 - accuracy: 0.8075 - val_loss: 0.3412 - val_accuracy: 0.8143\n",
      "Epoch 12/100\n",
      "3579/3579 - 0s - loss: 0.3436 - accuracy: 0.8156 - val_loss: 0.3388 - val_accuracy: 0.8300\n",
      "Epoch 13/100\n",
      "3579/3579 - 0s - loss: 0.3413 - accuracy: 0.8108 - val_loss: 0.3318 - val_accuracy: 0.8322\n",
      "Epoch 14/100\n",
      "3579/3579 - 0s - loss: 0.3381 - accuracy: 0.8187 - val_loss: 0.3302 - val_accuracy: 0.8300\n",
      "Epoch 15/100\n",
      "3579/3579 - 0s - loss: 0.3355 - accuracy: 0.8192 - val_loss: 0.3330 - val_accuracy: 0.8322\n",
      "Epoch 16/100\n",
      "3579/3579 - 0s - loss: 0.3364 - accuracy: 0.8122 - val_loss: 0.3327 - val_accuracy: 0.8367\n",
      "Epoch 17/100\n",
      "3579/3579 - 0s - loss: 0.3350 - accuracy: 0.8136 - val_loss: 0.3256 - val_accuracy: 0.8345\n",
      "Epoch 18/100\n",
      "3579/3579 - 0s - loss: 0.3301 - accuracy: 0.8217 - val_loss: 0.3254 - val_accuracy: 0.8345\n",
      "Epoch 19/100\n",
      "3579/3579 - 0s - loss: 0.3311 - accuracy: 0.8201 - val_loss: 0.3259 - val_accuracy: 0.8389\n",
      "Epoch 20/100\n",
      "3579/3579 - 0s - loss: 0.3290 - accuracy: 0.8181 - val_loss: 0.3218 - val_accuracy: 0.8389\n",
      "Epoch 21/100\n",
      "3579/3579 - 0s - loss: 0.3296 - accuracy: 0.8170 - val_loss: 0.3171 - val_accuracy: 0.8255\n",
      "Epoch 22/100\n",
      "3579/3579 - 0s - loss: 0.3305 - accuracy: 0.8206 - val_loss: 0.3195 - val_accuracy: 0.8255\n",
      "Epoch 23/100\n",
      "3579/3579 - 0s - loss: 0.3277 - accuracy: 0.8229 - val_loss: 0.3311 - val_accuracy: 0.8322\n",
      "Epoch 24/100\n",
      "3579/3579 - 0s - loss: 0.3276 - accuracy: 0.8181 - val_loss: 0.3168 - val_accuracy: 0.8345\n",
      "Epoch 25/100\n",
      "3579/3579 - 0s - loss: 0.3257 - accuracy: 0.8206 - val_loss: 0.3308 - val_accuracy: 0.8255\n",
      "Epoch 26/100\n",
      "3579/3579 - 0s - loss: 0.3251 - accuracy: 0.8237 - val_loss: 0.3179 - val_accuracy: 0.8345\n",
      "Epoch 27/100\n",
      "3579/3579 - 0s - loss: 0.3264 - accuracy: 0.8206 - val_loss: 0.3148 - val_accuracy: 0.8367\n",
      "Epoch 28/100\n",
      "3579/3579 - 0s - loss: 0.3228 - accuracy: 0.8248 - val_loss: 0.3173 - val_accuracy: 0.8300\n",
      "Epoch 29/100\n",
      "3579/3579 - 0s - loss: 0.3251 - accuracy: 0.8136 - val_loss: 0.3256 - val_accuracy: 0.8233\n",
      "Epoch 30/100\n",
      "3579/3579 - 0s - loss: 0.3205 - accuracy: 0.8206 - val_loss: 0.3171 - val_accuracy: 0.8300\n",
      "Epoch 31/100\n",
      "3579/3579 - 0s - loss: 0.3222 - accuracy: 0.8201 - val_loss: 0.3171 - val_accuracy: 0.8345\n",
      "Epoch 32/100\n",
      "3579/3579 - 0s - loss: 0.3196 - accuracy: 0.8226 - val_loss: 0.3133 - val_accuracy: 0.8300\n",
      "Epoch 33/100\n",
      "3579/3579 - 0s - loss: 0.3203 - accuracy: 0.8231 - val_loss: 0.3166 - val_accuracy: 0.8389\n",
      "Epoch 34/100\n",
      "3579/3579 - 0s - loss: 0.3222 - accuracy: 0.8234 - val_loss: 0.3191 - val_accuracy: 0.8233\n",
      "Epoch 35/100\n",
      "3579/3579 - 0s - loss: 0.3193 - accuracy: 0.8220 - val_loss: 0.3116 - val_accuracy: 0.8367\n",
      "Epoch 36/100\n",
      "3579/3579 - 0s - loss: 0.3198 - accuracy: 0.8245 - val_loss: 0.3262 - val_accuracy: 0.8255\n",
      "Epoch 37/100\n",
      "3579/3579 - 0s - loss: 0.3212 - accuracy: 0.8243 - val_loss: 0.3224 - val_accuracy: 0.8367\n",
      "Epoch 38/100\n",
      "3579/3579 - 0s - loss: 0.3201 - accuracy: 0.8187 - val_loss: 0.3103 - val_accuracy: 0.8345\n",
      "Epoch 39/100\n",
      "3579/3579 - 0s - loss: 0.3196 - accuracy: 0.8234 - val_loss: 0.3204 - val_accuracy: 0.8233\n",
      "Epoch 40/100\n",
      "3579/3579 - 0s - loss: 0.3182 - accuracy: 0.8284 - val_loss: 0.3258 - val_accuracy: 0.8188\n",
      "Epoch 41/100\n",
      "3579/3579 - 0s - loss: 0.3208 - accuracy: 0.8254 - val_loss: 0.3167 - val_accuracy: 0.8479\n",
      "Epoch 42/100\n",
      "3579/3579 - 0s - loss: 0.3184 - accuracy: 0.8220 - val_loss: 0.3138 - val_accuracy: 0.8367\n",
      "Epoch 43/100\n",
      "3579/3579 - 0s - loss: 0.3164 - accuracy: 0.8268 - val_loss: 0.3112 - val_accuracy: 0.8389\n",
      "Epoch 44/100\n",
      "3579/3579 - 0s - loss: 0.3153 - accuracy: 0.8270 - val_loss: 0.3114 - val_accuracy: 0.8434\n",
      "Epoch 45/100\n",
      "3579/3579 - 0s - loss: 0.3145 - accuracy: 0.8276 - val_loss: 0.3164 - val_accuracy: 0.8300\n",
      "Epoch 46/100\n",
      "3579/3579 - 0s - loss: 0.3192 - accuracy: 0.8195 - val_loss: 0.3111 - val_accuracy: 0.8434\n",
      "Epoch 47/100\n",
      "3579/3579 - 0s - loss: 0.3162 - accuracy: 0.8312 - val_loss: 0.3243 - val_accuracy: 0.8188\n",
      "Epoch 48/100\n",
      "3579/3579 - 0s - loss: 0.3186 - accuracy: 0.8209 - val_loss: 0.3118 - val_accuracy: 0.8456\n",
      "Epoch 49/100\n",
      "3579/3579 - 0s - loss: 0.3175 - accuracy: 0.8276 - val_loss: 0.3107 - val_accuracy: 0.8322\n",
      "Epoch 50/100\n",
      "3579/3579 - 0s - loss: 0.3232 - accuracy: 0.8184 - val_loss: 0.3080 - val_accuracy: 0.8412\n",
      "Epoch 51/100\n",
      "3579/3579 - 0s - loss: 0.3134 - accuracy: 0.8262 - val_loss: 0.3080 - val_accuracy: 0.8389\n",
      "Epoch 52/100\n",
      "3579/3579 - 0s - loss: 0.3137 - accuracy: 0.8270 - val_loss: 0.3124 - val_accuracy: 0.8456\n",
      "Epoch 53/100\n",
      "3579/3579 - 0s - loss: 0.3142 - accuracy: 0.8229 - val_loss: 0.3172 - val_accuracy: 0.8345\n",
      "Epoch 54/100\n",
      "3579/3579 - 0s - loss: 0.3187 - accuracy: 0.8220 - val_loss: 0.3072 - val_accuracy: 0.8434\n",
      "Epoch 55/100\n",
      "3579/3579 - 0s - loss: 0.3149 - accuracy: 0.8265 - val_loss: 0.3444 - val_accuracy: 0.8076\n",
      "Epoch 56/100\n",
      "3579/3579 - 0s - loss: 0.3213 - accuracy: 0.8265 - val_loss: 0.3114 - val_accuracy: 0.8367\n",
      "Epoch 57/100\n",
      "3579/3579 - 0s - loss: 0.3172 - accuracy: 0.8223 - val_loss: 0.3303 - val_accuracy: 0.8076\n",
      "Epoch 58/100\n",
      "3579/3579 - 0s - loss: 0.3178 - accuracy: 0.8237 - val_loss: 0.3053 - val_accuracy: 0.8322\n",
      "Epoch 59/100\n",
      "3579/3579 - 0s - loss: 0.3156 - accuracy: 0.8226 - val_loss: 0.3146 - val_accuracy: 0.8389\n",
      "Epoch 60/100\n",
      "3579/3579 - 0s - loss: 0.3138 - accuracy: 0.8265 - val_loss: 0.3099 - val_accuracy: 0.8456\n",
      "Epoch 61/100\n",
      "3579/3579 - 0s - loss: 0.3113 - accuracy: 0.8270 - val_loss: 0.3135 - val_accuracy: 0.8389\n",
      "Epoch 62/100\n",
      "3579/3579 - 0s - loss: 0.3154 - accuracy: 0.8273 - val_loss: 0.3153 - val_accuracy: 0.8345\n",
      "Epoch 63/100\n",
      "3579/3579 - 0s - loss: 0.3134 - accuracy: 0.8284 - val_loss: 0.3054 - val_accuracy: 0.8322\n",
      "Epoch 64/100\n",
      "3579/3579 - 0s - loss: 0.3141 - accuracy: 0.8265 - val_loss: 0.3258 - val_accuracy: 0.8188\n",
      "Epoch 65/100\n",
      "3579/3579 - 0s - loss: 0.3135 - accuracy: 0.8231 - val_loss: 0.3127 - val_accuracy: 0.8367\n",
      "Epoch 66/100\n",
      "3579/3579 - 0s - loss: 0.3113 - accuracy: 0.8282 - val_loss: 0.3116 - val_accuracy: 0.8434\n",
      "Epoch 67/100\n",
      "3579/3579 - 0s - loss: 0.3143 - accuracy: 0.8270 - val_loss: 0.3103 - val_accuracy: 0.8412\n",
      "Epoch 68/100\n",
      "3579/3579 - 0s - loss: 0.3123 - accuracy: 0.8248 - val_loss: 0.3127 - val_accuracy: 0.8389\n",
      "Epoch 69/100\n",
      "3579/3579 - 0s - loss: 0.3116 - accuracy: 0.8259 - val_loss: 0.3076 - val_accuracy: 0.8367\n",
      "Epoch 70/100\n",
      "3579/3579 - 0s - loss: 0.3126 - accuracy: 0.8234 - val_loss: 0.3067 - val_accuracy: 0.8367\n",
      "Epoch 71/100\n",
      "3579/3579 - 0s - loss: 0.3185 - accuracy: 0.8187 - val_loss: 0.3096 - val_accuracy: 0.8456\n",
      "Epoch 72/100\n",
      "3579/3579 - 0s - loss: 0.3122 - accuracy: 0.8229 - val_loss: 0.3247 - val_accuracy: 0.8210\n",
      "Epoch 73/100\n",
      "3579/3579 - 0s - loss: 0.3126 - accuracy: 0.8237 - val_loss: 0.3101 - val_accuracy: 0.8434\n",
      "Epoch 74/100\n",
      "3579/3579 - 0s - loss: 0.3116 - accuracy: 0.8287 - val_loss: 0.3107 - val_accuracy: 0.8367\n",
      "Epoch 75/100\n",
      "3579/3579 - 0s - loss: 0.3106 - accuracy: 0.8273 - val_loss: 0.3198 - val_accuracy: 0.8233\n",
      "Epoch 76/100\n",
      "3579/3579 - 0s - loss: 0.3144 - accuracy: 0.8254 - val_loss: 0.3091 - val_accuracy: 0.8412\n",
      "Epoch 77/100\n",
      "3579/3579 - 0s - loss: 0.3150 - accuracy: 0.8270 - val_loss: 0.3119 - val_accuracy: 0.8300\n",
      "Epoch 78/100\n",
      "3579/3579 - 0s - loss: 0.3105 - accuracy: 0.8284 - val_loss: 0.3111 - val_accuracy: 0.8412\n",
      "Epoch 79/100\n",
      "3579/3579 - 0s - loss: 0.3113 - accuracy: 0.8245 - val_loss: 0.3074 - val_accuracy: 0.8345\n",
      "Epoch 80/100\n",
      "3579/3579 - 0s - loss: 0.3094 - accuracy: 0.8284 - val_loss: 0.3105 - val_accuracy: 0.8412\n",
      "Epoch 81/100\n",
      "3579/3579 - 0s - loss: 0.3133 - accuracy: 0.8256 - val_loss: 0.3095 - val_accuracy: 0.8412\n",
      "Epoch 82/100\n",
      "3579/3579 - 0s - loss: 0.3087 - accuracy: 0.8293 - val_loss: 0.3150 - val_accuracy: 0.8300\n",
      "Epoch 83/100\n",
      "3579/3579 - 0s - loss: 0.3144 - accuracy: 0.8217 - val_loss: 0.3080 - val_accuracy: 0.8255\n",
      "Epoch 84/100\n",
      "3579/3579 - 0s - loss: 0.3101 - accuracy: 0.8284 - val_loss: 0.3163 - val_accuracy: 0.8345\n",
      "Epoch 85/100\n",
      "3579/3579 - 0s - loss: 0.3092 - accuracy: 0.8237 - val_loss: 0.3189 - val_accuracy: 0.8255\n",
      "Epoch 86/100\n",
      "3579/3579 - 0s - loss: 0.3118 - accuracy: 0.8298 - val_loss: 0.3190 - val_accuracy: 0.8277\n",
      "Epoch 87/100\n",
      "3579/3579 - 0s - loss: 0.3086 - accuracy: 0.8203 - val_loss: 0.3061 - val_accuracy: 0.8434\n",
      "Epoch 88/100\n",
      "3579/3579 - 0s - loss: 0.3137 - accuracy: 0.8217 - val_loss: 0.3100 - val_accuracy: 0.8456\n",
      "Epoch 89/100\n",
      "3579/3579 - 0s - loss: 0.3122 - accuracy: 0.8220 - val_loss: 0.3216 - val_accuracy: 0.8210\n",
      "Epoch 90/100\n",
      "3579/3579 - 0s - loss: 0.3105 - accuracy: 0.8332 - val_loss: 0.3139 - val_accuracy: 0.8389\n",
      "Epoch 91/100\n",
      "3579/3579 - 0s - loss: 0.3130 - accuracy: 0.8251 - val_loss: 0.3151 - val_accuracy: 0.8300\n",
      "Epoch 92/100\n",
      "3579/3579 - 0s - loss: 0.3212 - accuracy: 0.8226 - val_loss: 0.3031 - val_accuracy: 0.8412\n",
      "Epoch 93/100\n",
      "3579/3579 - 0s - loss: 0.3093 - accuracy: 0.8304 - val_loss: 0.3283 - val_accuracy: 0.8166\n",
      "Epoch 94/100\n",
      "3579/3579 - 0s - loss: 0.3150 - accuracy: 0.8184 - val_loss: 0.3086 - val_accuracy: 0.8412\n",
      "Epoch 95/100\n",
      "3579/3579 - 0s - loss: 0.3094 - accuracy: 0.8301 - val_loss: 0.3071 - val_accuracy: 0.8367\n",
      "Epoch 96/100\n",
      "3579/3579 - 0s - loss: 0.3078 - accuracy: 0.8307 - val_loss: 0.3069 - val_accuracy: 0.8322\n",
      "Epoch 97/100\n",
      "3579/3579 - 0s - loss: 0.3082 - accuracy: 0.8290 - val_loss: 0.3035 - val_accuracy: 0.8345\n",
      "Epoch 98/100\n",
      "3579/3579 - 0s - loss: 0.3073 - accuracy: 0.8343 - val_loss: 0.3064 - val_accuracy: 0.8345\n",
      "Epoch 99/100\n",
      "3579/3579 - 0s - loss: 0.3092 - accuracy: 0.8268 - val_loss: 0.3038 - val_accuracy: 0.8367\n",
      "Epoch 100/100\n",
      "3579/3579 - 0s - loss: 0.3106 - accuracy: 0.8290 - val_loss: 0.3119 - val_accuracy: 0.8188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d59aa02080>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# That's where we train the model we have built\n",
    "# set the batch size\n",
    "batch_size = 100\n",
    "\n",
    "# set a maximum number of training epochs\n",
    "max_epochs = 100\n",
    "\n",
    "model.fit(train_inputs, # train inputs\n",
    "          train_targets, # train targets\n",
    "          batch_size=batch_size, # batch size\n",
    "          epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)\n",
    "          validation_data=(validation_inputs, validation_targets), # validation data\n",
    "          verbose = 2) # making sure we get enough information about the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d9c7f3",
   "metadata": {},
   "source": [
    "### WHEN YOU TRAIN FOR LONG THERE IS HIGH CHANCE OF OVERFITTING. THIS IS EVIDENT WHEN THE TRAINING LOSS IS CONSTANTLY DECREASING\n",
    "### AND VALIDATION LOSS IS SOMETIME INCREASING. A FIX IS SETTING EARLY STOPING MECHNISM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a76a0",
   "metadata": {},
   "source": [
    "### VID 394 BUSINESS CASE. SETTING AN EARLY STOPPING MECHNISM WITH TENSORFLOW\n",
    "### THE FIT METHOD CONTAINS AN ARGUEMENT CALLED CALLBACKS\n",
    "THERE ARE DIFERRENT TYPES OF CALLBACKS BUT WE WILL USE THE \"EARLY STOPPING\". IT IS CALLED AT CERTAIN POINT DURING TRAINING\n",
    "EACH TIME THE VALIDATION LOSS IS CALCULATED IT IS COMPARED TO THE VAL. LOSS ONE EPOCH EGO, AND IF IT STARTS INCREASING THE MODEL IS OVERFITTING AND WE SHOULD STOP TRAINING.\n",
    "\n",
    "EARLY STOPING MECHNISM IS A HYPER PARAMETER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddc0210",
   "metadata": {},
   "source": [
    "## setting an early stopping mechanism and repeating the TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7454bab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3579 samples, validate on 447 samples\n",
      "Epoch 1/100\n",
      "3579/3579 - 0s - loss: 0.3098 - accuracy: 0.8212 - val_loss: 0.3238 - val_accuracy: 0.8166\n",
      "Epoch 2/100\n",
      "3579/3579 - 0s - loss: 0.3133 - accuracy: 0.8212 - val_loss: 0.3188 - val_accuracy: 0.8389\n",
      "Epoch 3/100\n",
      "3579/3579 - 0s - loss: 0.3101 - accuracy: 0.8296 - val_loss: 0.3186 - val_accuracy: 0.8121\n",
      "Epoch 4/100\n",
      "3579/3579 - 0s - loss: 0.3138 - accuracy: 0.8282 - val_loss: 0.3052 - val_accuracy: 0.8434\n",
      "Epoch 5/100\n",
      "3579/3579 - 0s - loss: 0.3074 - accuracy: 0.8310 - val_loss: 0.3101 - val_accuracy: 0.8434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d5915a0780>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# That's where we train the model we have built\n",
    "# set the batch size\n",
    "batch_size = 100\n",
    "\n",
    "# set a maximum number of training epochs\n",
    "max_epochs = 100\n",
    "\n",
    "# set an early stopping mechanism\n",
    "# let's set patience=2, to be a bit tolerant against random validation loss increases\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping()\n",
    "                                                  \n",
    "model.fit(train_inputs, # train inputs\n",
    "          train_targets, # train targets\n",
    "          batch_size=batch_size, # batch size\n",
    "          epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)\n",
    "          # callbacks are functions called by a task when a task is completed\n",
    "          # task here is to check if val_loss is increasing\n",
    "          callbacks=[early_stopping], # early stopping\n",
    "          validation_data=(validation_inputs, validation_targets), # validation data\n",
    "          verbose = 2) # making sure we get enough information about the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe60f394",
   "metadata": {},
   "source": [
    "#### SOMETIMES IF WE NOTICE THAT THE VALIDATION LOSS HAS INCREASED BY AN INSIGNIFICANT AMOUNT WE MAY PREFER TO LET ONE OR TWO \n",
    "#### VALIDATION INCREASES SLIDE. TO ALLOW FOR THIS TOLERANCE, WE CAN ADJUST THE EARLY STOPPING OBJECT. AN ARGUEMENT CALLED PATIENCE WHICH BY DEFAULT IS SET TO ZERO\n",
    "#### tf.keras.callbacks.EarlyStopping(patience)-configures the early stopping mechnism of the algo while patience lets us \n",
    "#### decide how many consecutive increases we can tolerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e830a39",
   "metadata": {},
   "source": [
    "## setting an early stopping mechanism with \"patience\" and repeating the TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aab954d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3579 samples, validate on 447 samples\n",
      "Epoch 1/100\n",
      "3579/3579 - 0s - loss: 0.3095 - accuracy: 0.8273 - val_loss: 0.3135 - val_accuracy: 0.8367\n",
      "Epoch 2/100\n",
      "3579/3579 - 0s - loss: 0.3099 - accuracy: 0.8284 - val_loss: 0.3151 - val_accuracy: 0.8389\n",
      "Epoch 3/100\n",
      "3579/3579 - 0s - loss: 0.3072 - accuracy: 0.8310 - val_loss: 0.3099 - val_accuracy: 0.8389\n",
      "Epoch 4/100\n",
      "3579/3579 - 0s - loss: 0.3083 - accuracy: 0.8296 - val_loss: 0.3028 - val_accuracy: 0.8367\n",
      "Epoch 5/100\n",
      "3579/3579 - 0s - loss: 0.3074 - accuracy: 0.8265 - val_loss: 0.3153 - val_accuracy: 0.8300\n",
      "Epoch 6/100\n",
      "3579/3579 - 0s - loss: 0.3081 - accuracy: 0.8310 - val_loss: 0.3093 - val_accuracy: 0.8367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d59a9bbb38>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# That's where we train the model we have built\n",
    "# set the batch size\n",
    "batch_size = 100\n",
    "\n",
    "# set a maximum number of training epochs\n",
    "max_epochs = 100\n",
    "\n",
    "# set an early stopping mechanism\n",
    "# let's set patience=2, to be a bit tolerant against random validation loss increases\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "                                                  \n",
    "model.fit(train_inputs, # train inputs\n",
    "          train_targets, # train targets\n",
    "          batch_size=batch_size, # batch size\n",
    "          epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)\n",
    "          # callbacks are functions called by a task when a task is completed\n",
    "          # task here is to check if val_loss is increasing\n",
    "          callbacks=[early_stopping], # early stopping\n",
    "          validation_data=(validation_inputs, validation_targets), # validation data\n",
    "          verbose = 2) # making sure we get enough information about the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6fafccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TAKE HOME POINTS\n",
    "# 1. Our priors were 50% and 50% so our algo learnt alot definitely. the final validation accuracy of the model is around 81%\n",
    "# it managed to classify around 81% of customers correctly. so if we are given 10 customers we will be able to correctly \n",
    "# identify the future cusomer behaviour of 8 of them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19acf600",
   "metadata": {},
   "source": [
    "### TEST THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "830a93ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 0s 645us/sample - loss: 0.3034 - accuracy: 0.8438\n"
     ]
    }
   ],
   "source": [
    "# It is very important to realize that fiddling with the hyperparameters overfits the validation dataset.\n",
    "# The test is the absolute final instance. You should not test before you are completely done with adjusting your model.\n",
    "#If you adjust your model after testing, you will start overfitting the test dataset, which will defeat its purpose.\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)\n",
    "\n",
    "# RECALL THAT EVALUATE RETURNS THE LOSS AND EVERY OTHER METRIC WE HAVE REQUESTED IN OUR MODEL OUTLINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "41013142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 0.30. Test accuracy: 84.38%\n"
     ]
    }
   ],
   "source": [
    "# lets print with some nice formatting\n",
    "print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))\n",
    "\n",
    "# this is the final accuracy of the model nd naturally it is close to the validation accuracy as we did not fiddle too much\n",
    "# with the hyperparameters\n",
    "# NOTE that u can get test accuracy higher than validation sometimes by luck, but theoritically it should be lower than or\n",
    "# equal to the validation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68214d5a",
   "metadata": {},
   "source": [
    "# tips to improve an algorithm\n",
    "1. improve the preprocessing\n",
    "2. Finetune the model by adjusting the width and depth of the algo. Also the number of hidden layers can be increased, hwever both the width and depth are computationally expensive . this can cause grt improvement \n",
    "3. play around with the activation fn\n",
    "4. feedle with the batch size\n",
    "   -batch size of 1 =SGD - here the algo will learn quickly but not so acurately\n",
    "   -Apply batch size that will likely preserve the underlying dependencies\n",
    "5. Experiment with the learning rate/ optimizer. although it may not be as fruitful as ADAM adapts it dynamically\n",
    " - visit the TF website and check out other optimizers comparing their performance to that of ADAM for ur specific problem\n",
    " - Alternatively check out tf.contribut. -thats where the tensorflow community contributes to the framework\n",
    "6. source for data eg in kaggle(kaggle dataset) nd implement this strategy to solve a given problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f6e42d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
